{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP3AFVMUeD+rZdjRSPzLV5S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benevolent-machines/crm-deduper/blob/main/docs/main-module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CRM Deduper Module\n",
        "\n",
        "See our repository for more information:\n",
        "\n",
        "https://github.com/benevolent-machines/crm-deduper\n"
      ],
      "metadata": {
        "id": "ACWPuMCsWng7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _temp\n",
        "A hidden global namespace for storing temporary copies of local variables that can be inspected outside of their scope.  "
      ],
      "metadata": {
        "id": "WI5l4BeZ-DZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class _temp: pass"
      ],
      "metadata": {
        "id": "Zovdihzb93KP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _time()\n",
        "\n",
        "A helper function that returns a string with the current time and elapsed time since the last call."
      ],
      "metadata": {
        "id": "3NfxzOqsd1ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_last = None\n",
        "def _time():\n",
        "    from datetime import datetime\n",
        "    global _last\n",
        "    now = datetime.now()\n",
        "    header = \"\"\n",
        "    if _last is None:\n",
        "        elapsed = \"00:00.000\"  # Initial call, no elapsed time\n",
        "    else:\n",
        "        delta = now - _last\n",
        "        minutes, seconds = divmod(delta.total_seconds(), 60)\n",
        "        milliseconds = delta.microseconds / 1000\n",
        "        elapsed = f\"{int(minutes):02}:{int(seconds):02}.{int(milliseconds):03}\"\n",
        "    _last = now\n",
        "    f_time = now.strftime('%M:%S.%f')[:-3]\n",
        "    return f\"{f_time} {elapsed}\""
      ],
      "metadata": {
        "id": "giTtxbhjdDUd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "if False:\n",
        "    import time\n",
        "    print(_time(),'first')  # First call, elapsed will be \"00:00.000\"\n",
        "    time.sleep(1.234)  # Sleep for a bit over a second to see the change\n",
        "    print(_time(), 'next')  # Second call, should show elapsed time since first call"
      ],
      "metadata": {
        "id": "SQ1iFwHRFfRf"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _similar()\n",
        "A helper function to quickly assess the similarity between two strings, useful for refining machine learning recall into higher precision levels. This Python code was adapted from a T-SQL function found here:\n",
        "\n",
        "https://www.sqlservercentral.com/articles/fuzzy-search"
      ],
      "metadata": {
        "id": "yvcZNeTKL_mI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _similar(s1, s2):\n",
        "    if len(s1) > len(s2):\n",
        "        (s1, s2) = (s2, s1) # easy switch in python!\n",
        "    k = len(s1)\n",
        "    if k < 2:\n",
        "        return 0.0\n",
        "    else:\n",
        "        j = 0\n",
        "        for i in range(len(s1) - 1):\n",
        "            substring = s1[i:i+2]\n",
        "            if substring in s2:\n",
        "                j += 1\n",
        "        return (j / (k-1)) * 100.0"
      ],
      "metadata": {
        "id": "pLRFFcu3ZbT7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "if False:\n",
        "    print(f\"{_similar('roger','susan')}\")\n",
        "    print(f\"{_similar('example','example')}\")"
      ],
      "metadata": {
        "id": "H-NEwJfc8lV5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _load_data()\n",
        "A helper function to load data from either a csv or excel file."
      ],
      "metadata": {
        "id": "0xtY_qFDYhZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _load_data(input_data):\n",
        "    import pandas as pd\n",
        "\n",
        "    if isinstance(input_data, str):\n",
        "        file_extension = input_data.split('.')[-1].lower()\n",
        "        if file_extension == 'csv':\n",
        "            df = pd.read_csv(input_data)\n",
        "        elif file_extension in ['xlsx', 'xls']:\n",
        "            df = pd.read_excel(input_data)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"load_data(): Unsupported file type: {file_extension}\")\n",
        "    elif isinstance(input_data, pd.DataFrame):\n",
        "        df = input_data\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"load_data(): Unsupported file type: {file_extension}\")\n",
        "\n",
        "    string_columns = df.select_dtypes(include=['object']).columns\n",
        "    df[string_columns] = df[string_columns].fillna('')\n",
        "    return df\n",
        "\n",
        "#_load_data(\"test.csv\")"
      ],
      "metadata": {
        "id": "vS0K-IvSY6_D"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _regroup_pass()\n",
        "A function designed to regroup rows during a deduplication pass for more even distribution, based on a specified threshold."
      ],
      "metadata": {
        "id": "l5vI5vmbXxj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _regroup_pass(df, col, num, threshold=2000):\n",
        "\n",
        "    # Slice the first `num` characters of `col` for grouping\n",
        "    sliced_col = df[col].str[:num].str.upper()\n",
        "\n",
        "    # Read sliced_col counts in descending order\n",
        "    col_counts = sliced_col.value_counts().sort_values(ascending=False)\n",
        "    accumulated = 0\n",
        "    group_id = 1  # Start sequence number for small groups\n",
        "    group_mapping = {}\n",
        "\n",
        "    for col_name, count in col_counts.items():\n",
        "        if count >= threshold:\n",
        "            # Categories larger than the threshold keep their original name\n",
        "            group_mapping[col_name] = col_name\n",
        "        else:\n",
        "            # Accumulate small categories until they meet the threshold\n",
        "            if accumulated + count <= threshold:\n",
        "                group_mapping[col_name] = f'_{group_id}'\n",
        "                accumulated += count\n",
        "            else:\n",
        "                # Once the threshold is exceeded, start a new group\n",
        "                group_id += 1\n",
        "                group_mapping[col_name] = f'_{group_id}'\n",
        "                accumulated = count  # Reset accumulated count for the new group\n",
        "\n",
        "    # Apply the group mapping based on the sliced_col values\n",
        "    df['group'] = sliced_col.map(group_mapping)\n",
        "    return df"
      ],
      "metadata": {
        "id": "IVV8uhfR_q38"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "if False:\n",
        "    df = _regroup_pass(df, 'state', 2, 3000)\n",
        "    group_sizes = df.groupby('group').size()\n",
        "    # Display count, maximum, and average of group sizes\n",
        "    print(f\"Total Groups: {group_sizes.count()}\")\n",
        "    print(f\"Maximum Group Size: {group_sizes.max()}\")\n",
        "    print(f\"Median Group Size: {group_sizes.median()}\")\n",
        "    df['group'].value_counts().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "4guPDKJYYD3F"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _find_duplicates()\n",
        "The main algorithm function for finding duplicates in large datasets."
      ],
      "metadata": {
        "id": "PYcf0ALXPtxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _find_duplicates(\n",
        "    pandas_dataframe, # required\n",
        "    entity_id = 'entity_id',\n",
        "    search_columns = ['name', 'address', 'city', 'state'],\n",
        "    dedupe_passes = ['ALL'],  # ['state:2', 'name:3']\n",
        "    similar_column = None,\n",
        "    score_minimum = 0.7,\n",
        "):\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "    df = pandas_dataframe\n",
        "\n",
        "    if dedupe_passes is None:\n",
        "        dedupe_passes = ['ALL']\n",
        "    elif isinstance(dedupe_passes, str):\n",
        "        dedupe_passes = [dedupe_passes]\n",
        "    elif not isinstance(dedupe_passes, list):\n",
        "        raise ValueError(\"invalid dedupe_passes parameter\")\n",
        "\n",
        "    pass_df_list = []\n",
        "    df['pass'] = None\n",
        "    df['group'] = None # initialize to avoid pandas warnings\n",
        "\n",
        "    for dedupe_pass in dedupe_passes: # ['state:2', 'city'] or 'ALL'\n",
        "        print(f\"{_time()} pass: {dedupe_pass}\")\n",
        "        pass_list = dedupe_pass.split(':')\n",
        "        pass_col = pass_list[0]\n",
        "        pass_len = int(pass_list[1]) if len(pass_list) > 1 else 2\n",
        "        df['pass'] = dedupe_pass\n",
        "        if pass_col == 'ALL':\n",
        "            df['group'] = 'ALL'\n",
        "        else:\n",
        "            df[pass_col] = df[pass_col].astype(str)\n",
        "            df = _regroup_pass(df, pass_col, pass_len) # updates df['group']\n",
        "\n",
        "        gdf_list = []\n",
        "\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            stop_words=None,\n",
        "            ngram_range=(1, 1), min_df=2, max_df=0.8)\n",
        "\n",
        "        for group, gdf in df.groupby('group'):\n",
        "            print(f\"{_time()} group: {group} shape[0]: {gdf.shape[0]}\")\n",
        "            gdf = gdf.sort_values(entity_id, ascending=False)\n",
        "            gdf = gdf.reset_index(drop=True)\n",
        "            gdf[search_columns] = gdf[search_columns].astype(str)\n",
        "            gdf['combined'] = gdf[search_columns].agg(' '.join, axis=1)\n",
        "            gdf['combined'] = gdf['combined'].str.upper()\n",
        "            gdf['last_id'] = gdf[entity_id]\n",
        "            gdf['dup_flag'] = 0\n",
        "            gdf['score'] = 0.0\n",
        "\n",
        "            tfidf_matrix = vectorizer.fit_transform(gdf['combined'])\n",
        "            print(f\"{_time()} vectorizer.fit_transform() completed\")\n",
        "\n",
        "            nbrs = NearestNeighbors(\n",
        "                radius=0.33, metric='cosine', algorithm='brute')\n",
        "            print(f\"{_time()} NearestNeighbors() completed\")\n",
        "\n",
        "            nbrs.fit(tfidf_matrix)\n",
        "            print(f\"{_time()} nbrs.fit() completed\")\n",
        "\n",
        "            distances, indices = nbrs.radius_neighbors(tfidf_matrix)\n",
        "            print(f\"{_time()} nbrs.radius_neighbors() completed\")\n",
        "\n",
        "            for i, (dist, idx) in enumerate(zip(distances, indices)):\n",
        "                for d, j in zip(dist, idx):\n",
        "                    if i < j:  # only compare with what comes after\n",
        "                        if (similar_column is not None\n",
        "                            and _similar(gdf.loc[i, similar_column],\n",
        "                                         gdf.loc[j, similar_column]) < 50\n",
        "                        ):\n",
        "                            continue; # not similar\n",
        "                        score = 1 - d  # Convert distance to similarity\n",
        "                        if score > score_minimum:\n",
        "                            if gdf.loc[i, 'dup_flag'] == 0:\n",
        "                                gdf.loc[i, 'dup_flag'] = 2\n",
        "                            gdf.loc[j, 'dup_flag'] = 1\n",
        "                            if gdf.loc[j, 'last_id'] == gdf.loc[j, entity_id]:\n",
        "                                gdf.loc[j, 'last_id'] = gdf.loc[i, entity_id]\n",
        "                                if gdf.loc[i, 'score'] == 0.0:\n",
        "                                    gdf.loc[i, 'score'] = 1.0\n",
        "                                gdf.loc[j, 'score'] = round(score, 3)\n",
        "\n",
        "            gdf_list.append(gdf)\n",
        "\n",
        "        pass_df = pd.concat(gdf_list, ignore_index=True)\n",
        "        #pass_df.drop(columns=['group'], inplace=True)\n",
        "\n",
        "        pass_df_list.append(pass_df[pass_df['dup_flag'].isin([1,2])]) # dups\n",
        "        df = pass_df[pass_df['dup_flag'] == 0].copy() # drop dups in next pass\n",
        "\n",
        "    results_df = pd.concat(pass_df_list, ignore_index=True)\n",
        "    #results_df.drop(columns=['group'], inplace=True)\n",
        "    #results_df = results_df.copy()\n",
        "    #m.df = df.copy()\n",
        "    all_df = pd.concat([df, results_df], ignore_index=True)\n",
        "    #all_df.to_csv(\"labeled_out.csv\")\n",
        "\n",
        "    _temp.audit_df = all_df\n",
        "\n",
        "    return all_df"
      ],
      "metadata": {
        "id": "wDoYnMdh5QKX"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _excel_redirect()\n",
        "A helper function to ensure that links in Excel can access CRM functions without being sent to a login screen each time."
      ],
      "metadata": {
        "id": "soRPGbSWT9Bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _excel_redirect(url):\n",
        "    redirector = \"https://benevolentmachines.org/excel_redirect.html\"\n",
        "    url = redirector + '?page=' + url.replace('?',\"&\")\n",
        "    return url"
      ],
      "metadata": {
        "id": "igOhXExTS0JA"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _link_templates\n",
        "A dictionary of dictionaries with templates used to construct links in the Excel output file.  "
      ],
      "metadata": {
        "id": "rgQkWSHE0Me8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_link_templates = {\n",
        "    \"donorperfect\": {\n",
        "        \"record\": \"https://www.donorperfect.net/prod/donor.asp\"\n",
        "            + \"?donor_id={entity_id}\",\n",
        "        \"merge\": \"https://www.donorperfect.net/prod/combinedonorsStart.asp\"\n",
        "            + \"?donor_id1={entity_id}&donor_id2={last_id}&combineType=Advanced\"\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "QUB6bBMQu8vz"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _output_excel()\n",
        "The function used to output found duplicates in a formatted Excel document.  "
      ],
      "metadata": {
        "id": "pmphUFtRj4-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test - The invoking context must install this package and datamachine to work.\n",
        "!pip install XlsxWriter"
      ],
      "metadata": {
        "id": "e_3-7rjjca_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8392c616-6ccb-47ef-cbda-9893402d0203"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: XlsxWriter in /usr/local/lib/python3.10/dist-packages (3.1.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _output_excel(\n",
        "    pandas_dataframe,\n",
        "    output_file='output.xlsx',\n",
        "    entity_id = 'entity_id',\n",
        "    search_columns = ['name', 'address', 'city', 'state'],\n",
        "    link_templates_key = None,\n",
        "    output_all = False, # True outputs all records, False is just the dups\n",
        "):\n",
        "    import pandas as pd\n",
        "    import xlsxwriter\n",
        "\n",
        "    # organize the columns\n",
        "    xdf = pandas_dataframe\n",
        "    xdf['type'] = xdf['dup_flag']\n",
        "    initial = ['type', entity_id, 'last_id', 'score']\n",
        "    reference = []  # additional fields\n",
        "    for col in xdf.columns: # extract references for the spreadsheet\n",
        "        if col not in initial and col not in search_columns:\n",
        "            if col != 'rownum' and col != 'combined':\n",
        "                reference.append(col)\n",
        "    ordered = initial + search_columns + reference\n",
        "    if link_templates_key is not None:\n",
        "        link_templates_key = link_templates_key.lower()\n",
        "\n",
        "    # prepare the dataframe for the spreadsheet\n",
        "    if output_all == False:\n",
        "        xdf = xdf[xdf['dup_flag'] > 0] # the spreadsheet only includes duplicates\n",
        "    xdf = xdf[ordered] # the order that will appear in the spreadsheet\n",
        "    xdf = xdf.sort_values(['last_id','dup_flag'], ascending=[False, False])\n",
        "    xdf = xdf.reset_index(drop=True) # renumber after sorting\n",
        "    xdf = xdf.fillna('') # important\n",
        "    #xdf.rename(columns={'dup_flag': 'type'}, inplace=True)\n",
        "    xdf['type'] = xdf['type'].map({2: 'last', 1: 'prior'}).astype(str)\n",
        "    xdf['type'] = xdf['type'].str.replace('nan','')\n",
        "\n",
        "    if link_templates_key is not None:\n",
        "        xdf.insert(loc=3, column='action', value='')\n",
        "\n",
        "    excel_file = output_file # 'output.xlsx'\n",
        "    writer = pd.ExcelWriter(excel_file, engine='xlsxwriter')\n",
        "\n",
        "    xdf.to_excel(writer, index=False, sheet_name='Sheet1', startrow=1, header=False)\n",
        "    workbook = writer.book\n",
        "    worksheet = writer.sheets['Sheet1']\n",
        "\n",
        "    # Custom formats\n",
        "    header_grey_left = workbook.add_format(\n",
        "        {'bg_color': '#D3D3D3', 'bold': True, 'align': 'left'})\n",
        "    header_grey_right = workbook.add_format(\n",
        "        {'bg_color': '#D3D3D3', 'bold': True, 'align': 'right'})\n",
        "    header_green_left = workbook.add_format(\n",
        "        {'bg_color': '#B8D7A3', 'bold': True, 'align': 'left'})\n",
        "\n",
        "    last_blue_left = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'left'})\n",
        "    last_blue_right = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'right'})\n",
        "    last_blue_right_score = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'right', 'num_format': '0.000'})\n",
        "    last_blue_right_date = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'right', 'num_format': 'yyyy-mm-dd'})\n",
        "    last_blue_right_dec2 = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'right', 'num_format': '#,##0.00'})\n",
        "    last_blue_right_dec3 = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'right', 'num_format': '##0.000'})\n",
        "    last_blue_left_link = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'left',\n",
        "        'font_color': 'blue', 'underline': 1})\n",
        "    last_blue_right_link = workbook.add_format(\n",
        "        {'bg_color': '#ADD8E6', 'align': 'right',\n",
        "        'font_color': 'blue', 'underline': 1})\n",
        "\n",
        "    prior_left = workbook.add_format({'align': 'left'})\n",
        "    prior_right = workbook.add_format({'align': 'right'})\n",
        "    prior_right_score = workbook.add_format(\n",
        "        {'align': 'right', 'num_format': '0.000'})\n",
        "    prior_right_date = workbook.add_format(\n",
        "        {'align': 'right', 'num_format': 'yyyy-mm-dd'})\n",
        "    prior_right_dec2 = workbook.add_format(\n",
        "        {'align': 'right', 'num_format': '#,##0.00'})\n",
        "    prior_right_dec3 = workbook.add_format(\n",
        "        {'align': 'right', 'num_format': '##0.000'}) # score\n",
        "    prior_left_link = workbook.add_format(\n",
        "        {'align': 'left', 'font_color': 'blue', 'underline': 1})\n",
        "    prior_right_link = workbook.add_format(\n",
        "        {'align': 'right', 'font_color': 'blue', 'underline': 1})\n",
        "\n",
        "    ##### determine column properties\n",
        "    tdf = xdf[0:500] # look at first n rows to extract properties\n",
        "    col_info = {} # info about the columns\n",
        "    for col in tdf.columns:\n",
        "        col_props = {}\n",
        "        is_numeric = is_string = is_date = False\n",
        "        max_decimals = 0\n",
        "        is_numeric = pd.to_numeric(\n",
        "            tdf[col].dropna(), errors='coerce').notnull().all()\n",
        "        if is_numeric:\n",
        "            numeric_series = pd.to_numeric(tdf[col], errors='coerce').dropna()\n",
        "            decimal_lengths = numeric_series.apply(\n",
        "                lambda x: len(str(x).split('.')[1]) if '.' in str(x) else 0)\n",
        "            max_decimals = decimal_lengths.max()\n",
        "        else:\n",
        "            is_date = pd.to_datetime(\n",
        "                tdf[col].dropna(), errors='coerce').notnull().all()\n",
        "            if not is_date:\n",
        "                is_string = tdf[col].dtype == 'object'\n",
        "        col_props['is_numeric'] = is_numeric\n",
        "        col_props['is_string'] = is_string\n",
        "        col_props['is_date'] = is_date\n",
        "        col_props['max_decimals'] = max_decimals\n",
        "        col_info[col] = col_props\n",
        "\n",
        "    # column settings\n",
        "    for num, col in enumerate(tdf.columns):\n",
        "\n",
        "        # Calculate the maximum length of data in the column\n",
        "        if tdf.empty:\n",
        "            max_data_len = 0\n",
        "        else:\n",
        "            max_data_len = tdf[col].astype(str).map(len).max()\n",
        "\n",
        "        # Consider the length of the column name as well\n",
        "        col_name_len = len(col)\n",
        "\n",
        "        # Determine the final column width\n",
        "        max_len = max(max_data_len, col_name_len) + 1  # +1 little extra\n",
        "        #print(col, max_len)\n",
        "        if max_len > 15:\n",
        "            max_len = 15\n",
        "        worksheet.set_column(num, num, max_len)\n",
        "\n",
        "        # custom headers\n",
        "        if col in search_columns: # comparison columns get a green header\n",
        "            worksheet.write(0, num, col, header_green_left)\n",
        "        else:\n",
        "            if col == 'type':\n",
        "                worksheet.write(0, num, col, header_grey_right)\n",
        "            elif col_info[col]['is_string']:\n",
        "                worksheet.write(0, num, col, header_grey_left)\n",
        "            else:\n",
        "                worksheet.write(0, num, col, header_grey_right)\n",
        "\n",
        "    # cell settings\n",
        "    for idx, row in xdf.iterrows():\n",
        "        row_num = idx + 1  # account for header row\n",
        "        for col_num, col in enumerate(tdf.columns):\n",
        "            if row['type'] == 'last':\n",
        "                if col == 'type':\n",
        "                    worksheet.write(row_num, col_num, row[col],\n",
        "                                    last_blue_right)\n",
        "                elif col == entity_id:\n",
        "                    if link_templates_key is not None: # hyperlinks\n",
        "                        url = _link_templates[\n",
        "                            link_templates_key][\"record\"].format(\n",
        "                                entity_id = row[entity_id]\n",
        "                            )\n",
        "                        url = _excel_redirect(url) # excel nonsense\n",
        "                        if col_info[col][\"is_string\"]:\n",
        "                            worksheet.write_url(row_num, col_num, url,\n",
        "                                last_blue_left_link,\n",
        "                                string=str(row[entity_id]))\n",
        "                        else:\n",
        "                            worksheet.write_url(row_num, col_num, url,\n",
        "                                last_blue_right_link,\n",
        "                                string=str(row[entity_id]))\n",
        "                    else:\n",
        "                        if col_info[col][\"is_string\"]:\n",
        "                            worksheet.write(\n",
        "                                row_num, col_num, row[col], last_blue_left)\n",
        "                        else:\n",
        "                            worksheet.write(\n",
        "                                row_num, col_num, row[col], last_blue_right)\n",
        "                elif col == 'action': # blank and blue\n",
        "                    worksheet.write(row_num, col_num, '', last_blue_right)\n",
        "                elif col == 'score': # blank and blue\n",
        "                    worksheet.write(row_num, col_num, '', last_blue_right)\n",
        "                else:\n",
        "                    if col_info[col][\"is_string\"]:\n",
        "                        worksheet.write(row_num, col_num, row[col],\n",
        "                                        last_blue_left)\n",
        "                    elif col_info[col]['is_numeric']:\n",
        "                        if col_info[col]['max_decimals'] == 2:\n",
        "                            worksheet.write(row_num, col_num, row[col],\n",
        "                                            last_blue_right_dec2)\n",
        "                        else:\n",
        "                            worksheet.write(row_num, col_num, row[col],\n",
        "                                            last_blue_right)\n",
        "                    elif col_info[col]['is_date']:\n",
        "                        worksheet.write(row_num, col_num, row[col],\n",
        "                                        last_blue_right_date)\n",
        "                    else: # other?\n",
        "                        worksheet.write(row_num, col_num, row[col],\n",
        "                                        last_blue_right)\n",
        "            else: # prior\n",
        "                if col == 'type':\n",
        "                    worksheet.write(row_num, col_num, row[col], prior_right)\n",
        "                elif col == entity_id:\n",
        "                    if link_templates_key is not None: # hyperlinks\n",
        "                        url = _link_templates[\n",
        "                            link_templates_key][\"record\"].format(\n",
        "                                entity_id = row[entity_id]\n",
        "                            )\n",
        "                        url = _excel_redirect(url) # excel nonsense\n",
        "                        if col_info[col][\"is_string\"]:\n",
        "                            worksheet.write_url(row_num, col_num, url,\n",
        "                                prior_left_link, string=str(row[entity_id]))\n",
        "                        else:\n",
        "                            worksheet.write_url(row_num, col_num, url,\n",
        "                                prior_right_link, string=str(row[entity_id]))\n",
        "                    else:\n",
        "                        if col_info[col][\"is_string\"]:\n",
        "                            worksheet.write(\n",
        "                                row_num, col_num, row[col], prior_left)\n",
        "                        else:\n",
        "                            worksheet.write(\n",
        "                                row_num, col_num, row[col], prior_right)\n",
        "                elif col == 'action':\n",
        "                    if link_templates_key is not None:\n",
        "                        url = _link_templates[\n",
        "                            link_templates_key][\"merge\"].format(\n",
        "                                entity_id = row[entity_id],\n",
        "                                last_id = row[\"last_id\"]\n",
        "                            )\n",
        "                        url = _excel_redirect(url) # excel nonsense\n",
        "                        worksheet.write_url(row_num, col_num, url,\n",
        "                            prior_left_link, string='merge')\n",
        "                elif col == 'score':\n",
        "                    worksheet.write(row_num, col_num, row[col],\n",
        "                                    prior_right_dec3)\n",
        "                else:\n",
        "                    if col_info[col][\"is_string\"]:\n",
        "                        #print(col)\n",
        "                        worksheet.write(row_num, col_num, row[col],\n",
        "                                        prior_left)\n",
        "                    elif col_info[col]['is_numeric']:\n",
        "                        if col_info[col]['max_decimals'] == 2:\n",
        "                            worksheet.write(row_num, col_num, row[col],\n",
        "                                            prior_right_dec2)\n",
        "                        else:\n",
        "                            worksheet.write(row_num, col_num, row[col],\n",
        "                                            prior_right)\n",
        "                    elif col_info[col]['is_date']:\n",
        "                        worksheet.write(row_num, col_num, row[col],\n",
        "                                        prior_right_date)\n",
        "                    else: # other?\n",
        "                        worksheet.write(row_num, col_num, row[col],\n",
        "                                        prior_right)\n",
        "\n",
        "    worksheet.freeze_panes(1, 4)\n",
        "    writer.close()\n",
        "    return xdf"
      ],
      "metadata": {
        "id": "xvgbS_wGXlMI"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run()"
      ],
      "metadata": {
        "id": "xufeH01u9B5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run(\n",
        "    input_data, # file (xls,xlsx,csv) or dataframe\n",
        "    entity_id = 'entity_id', # unique id needed for linking\n",
        "    search_columns = ['first_name', 'last_name', 'address', 'city', 'state'],\n",
        "    similar_column = None, # 'first_name' could exclude household matches\n",
        "    amount_column = None, # 'total_amount', for instance\n",
        "    amount_minimum = None, # the minimum amount to isolate major pairs\n",
        "    recent_column = None, # 'insert_date' could be used for recent records\n",
        "    recent_days = None, # the number of recent days with recent_column\n",
        "    string_column = None, # can be 'created_by' for source\n",
        "    string_equals = None, # the value used with string_column\n",
        "    exclude_previous = False, # uses previous.xlsx to exclude matches\n",
        "    link_templates_key = None, # templates for CRM record and merge links\n",
        "    # the following parameters are experimental\n",
        "    output_all = False, # True outputs all records, False is just the dups\n",
        "    score_minimum = .67,\n",
        "    dedupe_passes = ['ALL'],  # ['state:2', 'name:3']\n",
        "    tuning = False,\n",
        "    quiet = False,\n",
        "    limit = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Runs deduplication on address data based on configurable parameters.\n",
        "\n",
        "    Documentation is available at:\n",
        "    https://github.com/benevolent-machines/crm-deduper\n",
        "\n",
        "    \"\"\"\n",
        "    global _last\n",
        "    _last = None\n",
        "    # suppress warning\n",
        "    import warnings\n",
        "    import dateutil\n",
        "    import os\n",
        "    import shutil\n",
        "    import pandas as pd\n",
        "\n",
        "    warnings.filterwarnings(\"ignore\",\n",
        "        category=dateutil.parser.UnknownTimezoneWarning)\n",
        "\n",
        "    if not quiet: print(f\"{_time()} loading data\")\n",
        "    df = _load_data(input_data) # load file\n",
        "    if limit is not None:\n",
        "        df = pd.DataFrame(df[0:limit])\n",
        "\n",
        "    if not quiet: print(f\"{_time()} finding duplicates\")\n",
        "    df = _find_duplicates(\n",
        "        df,\n",
        "        entity_id = entity_id,\n",
        "        search_columns = search_columns,\n",
        "        dedupe_passes = dedupe_passes,\n",
        "        score_minimum = .67,\n",
        "        similar_column = similar_column,\n",
        "    )\n",
        "\n",
        "    # limit the output to pairs including the largest amounts\n",
        "    if amount_column is not None and amount_minimum is not None:\n",
        "        # filter on duplications involving amount_field at a amount_minimum\n",
        "        last_ids = list(\n",
        "            df[df[amount_column] >= amount_minimum]['last_id'].unique())\n",
        "        df = df[df['last_id'].isin(last_ids)].copy()\n",
        "\n",
        "    # limit the output to pairs with recent activity\n",
        "    if recent_column is not None and recent_days is not None:\n",
        "        df[recent_column] = pd.to_datetime(df[recent_column]) # convert\n",
        "        cutoff_date = pd.Timestamp.now() - pd.Timedelta(days=recent_days)\n",
        "        last_ids = list(\n",
        "            df[df[recent_column] >= cutoff_date]['last_id'].unique())\n",
        "        df = df[df['last_id'].isin(last_ids)].copy()\n",
        "\n",
        "    # limit the output to pairs including a specific string value\n",
        "    if string_column is not None and string_equals is not None:\n",
        "        last_ids = list(\n",
        "            df[df[string_column] >= string_equals]['last_id'].unique())\n",
        "        df = df[df['last_id'].isin(last_ids)].copy()\n",
        "\n",
        "    if tuning:\n",
        "        return df # run results for tuning and avoid excel outputs\n",
        "    else:\n",
        "        if exclude_previous:\n",
        "            if os.path.isfile('previous.xlsx') and not output_all:\n",
        "                prev_df = pd.read_excel('previous.xlsx')\n",
        "                c1 = ~df[entity_id].isin(prev_df[entity_id])\n",
        "                df = df[c1].copy() # remove previous\n",
        "\n",
        "        if not quiet: print(f\"{_time()} outputting Excel\")\n",
        "        xdf = _output_excel(\n",
        "            df,\n",
        "            entity_id=entity_id,\n",
        "            search_columns=search_columns,\n",
        "            link_templates_key=link_templates_key,\n",
        "            output_all=output_all,\n",
        "        )\n",
        "        if not quiet: print(f\"{_time()} results completed in output.xlsx\")\n",
        "\n",
        "        if exclude_previous and os.path.isfile('previous.xlsx'):\n",
        "            prev_df.drop(['action', 'type'], axis=1, inplace=True)\n",
        "            c1 = df['dup_flag'].isin([1,2]) # duplicates\n",
        "            prev_df = pd.concat([df[c1], prev_df]).copy() # new previous\n",
        "            xprev_df = _output_excel(\n",
        "                prev_df, output_file='previous.xlsx',\n",
        "                entity_id=entity_id,\n",
        "                search_columns=search_columns,\n",
        "                link_templates_key=link_templates_key,\n",
        "                output_all=output_all,\n",
        "            )\n",
        "        else: # initialize the previous as the output\n",
        "            shutil.copy('output.xlsx','previous.xlsx')\n",
        "\n",
        "        return xdf # formatted results for users - also in output.xlsx"
      ],
      "metadata": {
        "id": "JnHWuUYC9RF0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#module_end  datamachine end of module"
      ],
      "metadata": {
        "id": "WkGzCK-AIGfx"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTS"
      ],
      "metadata": {
        "id": "7ZybEnjd9Lnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CRM Data (CSV)"
      ],
      "metadata": {
        "id": "OiberZ4D30rP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "    df = run(\n",
        "        input_data = \"donors.csv\", # exported from the CRM\n",
        "        entity_id = \"donor_id\", # unique id\n",
        "        search_columns = ['first_name', 'last_name', 'address', 'city', 'state'],\n",
        "        dedupe_passes = ['ALL'],\n",
        "        link_templates_key = \"donorperfect\",\n",
        "        similar_column = 'first_name',\n",
        "        #recent_column = 'last_contrib_date',\n",
        "        #recent_days = 7,\n",
        "        amount_column = 'gift_total',\n",
        "        amount_minimum = 1_000,\n",
        "        output_all = False,\n",
        "        tuning = False,\n",
        "        quiet = False,\n",
        "        #limit = 10_000,\n",
        "    )"
      ],
      "metadata": {
        "id": "bi80Ma1e3LBI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63e2384f-200c-4a93-f269-0de04be10fc5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21:10.936 00:00.000 loading data\n",
            "21:11.280 00:00.344 finding duplicates\n",
            "21:12.809 00:01.528 pass: ALL\n",
            "21:12.846 00:00.036 group: ALL shape[0]: 75232\n",
            "21:14.013 00:01.166 vectorizer.fit_transform() completed\n",
            "21:14.013 00:00.000 NearestNeighbors() completed\n",
            "21:14.016 00:00.002 nbrs.fit() completed\n",
            "22:46.176 01:32.160 nbrs.radius_neighbors() completed\n",
            "22:46.945 00:00.768 outputting Excel\n",
            "22:47.105 00:00.159 results completed in output.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TUNING"
      ],
      "metadata": {
        "id": "RY_lsERp6kPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _get_metrics()"
      ],
      "metadata": {
        "id": "IASt2js9QcNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_metrics(df, quiet=False):\n",
        "    import pandas as pd\n",
        "    audit_df = df\n",
        "    audit_df['dup_flag'] = audit_df['dup_flag'].fillna(0)\n",
        "    audit_df['label'] = audit_df['label'].fillna(0)\n",
        "    # First, let's define what constitutes a true positive, false positive, and false negative in your context\n",
        "    # True Positive (TP): dup_flag is 1 or 2 AND label is 1 or 2\n",
        "    # False Positive (FP): dup_flag is 1 or 2 BUT label is NOT 1 or 2\n",
        "    # False Negative (FN): dup_flag is NOT 1 or 2 BUT label is 1 or 2\n",
        "    total = audit_df.shape[0]\n",
        "    labeled = audit_df[audit_df['label'].isin([1, 2])].shape[0]\n",
        "    found = audit_df[audit_df['dup_flag'].isin([1, 2])].shape[0]\n",
        "    TP = audit_df[(audit_df['dup_flag'].isin([1, 2])) & (audit_df['label'].isin([1, 2]))].shape[0]\n",
        "    FP = audit_df[(audit_df['dup_flag'].isin([1, 2])) & (~audit_df['label'].isin([1, 2]))].shape[0]\n",
        "    FN = audit_df[(~audit_df['dup_flag'].isin([1, 2])) & (audit_df['label'].isin([1, 2]))].shape[0]\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    if not quiet:\n",
        "        print(f'total: {total}, labeled: {labeled}, found {found}')\n",
        "        print(f\"Precision: {precision*100:.2f}% : Of those found, the percent that were labeled\")\n",
        "        print(f\"   Recall: {recall*100:.2f}% : Of those labeled, the percent that were found.\")\n",
        "    return (total, labeled, found, precision, recall)\n",
        "#_get_metrics(df)"
      ],
      "metadata": {
        "id": "Y5qmDhEodGEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tuning()"
      ],
      "metadata": {
        "id": "bSROihVfQhkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if False:\n",
        "    import pandas as pd\n",
        "    from io import StringIO\n",
        "    import ast\n",
        "    import time\n",
        "    zdf = pd.read_csv(StringIO( \"\"\"\\\n",
        "    secs|precision|recall|file|passes\n",
        "    0|0.0|0.0|labels2.csv|['first_name:2']\n",
        "    0|0.0|0.0|labels2.csv|['last_name:2']\n",
        "    0|0.0|0.0|labels2.csv|['last_name:2']\n",
        "    0|0.0|0.0|labels2.csv|['state:2']\n",
        "    0|0.0|0.0|labels2.csv|['ALL']\n",
        "    0|0.0|0.0|labels2.csv|['state:2','ALL']\n",
        "    \"\"\"), delimiter='|',\n",
        "    dtype={})\n",
        "    for index, row in zdf.iterrows():\n",
        "        start_time = round(time.time())\n",
        "\n",
        "        print(f'test:{index} file:{row[\"file\"]} passes:{row[\"passes\"]}')\n",
        "        passes = ast.literal_eval(row['passes'])\n",
        "        df = run(\n",
        "            input_data = row['file'], # exported from the CRM\n",
        "            entity_id = \"donor_id\", # unique id\n",
        "            search_columns = ['first_name', 'last_name', 'address', 'city', 'state'],\n",
        "            dedupe_passes = passes,\n",
        "            link_templates = \"donorperfect\",\n",
        "            output_all = True, # testing\n",
        "            tuning = True,\n",
        "            quiet = True,\n",
        "        )\n",
        "        (total, labeled, found, precision, recall) = _get_metrics(df, quiet=True)\n",
        "        zdf.at[index, 'precision'] = precision\n",
        "        zdf.at[index, 'recall'] = recall\n",
        "        zdf.at[index, 'secs'] = round(time.time()) - start_time\n",
        "    zdf"
      ],
      "metadata": {
        "id": "RX0x6gDMAriI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}